#include "Spider.h"
#include <chrono>

Spider::Spider(Config& config, Database& db)
    : config_(config)
    , database_(db)
    , stopRequested_(false)
    , activeWorkers_(0)
    , pagesDownloaded_(0)
    , pagesIndexed_(0)
{
    // –ù–∞—á–∏–Ω–∞–µ–º —Å–æ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    addTask(config_.getSpiderStartUrl(), 0);
}

Spider::~Spider()
{
    stop();
}

void Spider::addTask(const std::string& url, int depth)
{
    std::unique_lock<std::mutex> lock(queueMutex_);

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω–∞ –ª–∏ –≥–ª—É–±–∏–Ω–∞
    if (depth > config_.getSpiderMaxDepth())
    {
        return;
    }

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –ª–∏ —É–∂–µ —ç—Ç–æ—Ç URL
    {
        std::lock_guard<std::mutex> processedLock(processedMutex_);
        if (processedUrls_.find(url) != processedUrls_.end())
        {
            return;
        }
    }

    // –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ –æ—á–µ—Ä–µ–¥—å
    taskQueue_.push({ url, depth });
    queueCV_.notify_one();
}

void Spider::workerFunction()
{
    activeWorkers_++;

    while (!stopRequested_)
    {
        DownloadTask task;

        {
            std::unique_lock<std::mutex> lock(queueMutex_);

            // –ñ–¥—ë–º –∑–∞–¥–∞—á—É –∏–ª–∏ –∫–æ–º–∞–Ω–¥—É –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            queueCV_.wait(lock, [this]() {
                return !taskQueue_.empty() || stopRequested_;
                });

            if (stopRequested_ && taskQueue_.empty())
            {
                break;
            }

            if (taskQueue_.empty())
            {
                continue;
            }

            // –ë–µ—Ä—ë–º –∑–∞–¥–∞—á—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏
            task = taskQueue_.front();
            taskQueue_.pop();
        }

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        try
        {
            processPage(task.url, task.depth);
        }
        catch (const std::exception& e)
        {
            std::cerr << "–û—à–∏–±–∫–∞ –≤ —Ä–∞–±–æ—á–µ–º –ø–æ—Ç–æ–∫–µ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ "
                << task.url << ": " << e.what() << std::endl;
        }
    }

    activeWorkers_--;
}

void Spider::processPage(const std::string& url, int depth)
{
    try
    {
        std::cout << "–ü–æ–ø—ã—Ç–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã [" << depth << "]: " << url << std::endl;

        // –ü–æ–º–µ—á–∞–µ–º URL –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–π
        {
            std::lock_guard<std::mutex> lock(processedMutex_);
            if (processedUrls_.find(url) != processedUrls_.end())
            {
                std::cout << "URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: " << url << std::endl;
                return;
            }
            processedUrls_.insert(url);
        }

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç –≤ –ë–î
        if (database_.urlExists(url))
        {
            std::cout << "–î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –ë–î: " << url << std::endl;
            return;
        }

        // –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        std::string html;
        try
        {
            std::cout << "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ: " << url << std::endl;
            html = downloader_.download(url);
            pagesDownloaded_++;
            std::cout << "–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: " << html.size() << " –±–∞–π—Ç" << std::endl;
        }
        catch (const std::exception& e)
        {
            std::cerr << "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ " << url << ": " << e.what() << std::endl;
            return;
        }

        // –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        Indexer::IndexingResult result;
        try
        {
            std::cout << "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: " << url << std::endl;
            result = indexer_.indexPage(html, url);
            pagesIndexed_++;
            std::cout << "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Å–ª–æ–≤: " << result.wordsFrequency.size() << std::endl;
        }
        catch (const std::exception& e)
        {
            std::cerr << "–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ " << url << ": " << e.what() << std::endl;
            return;
        }

        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        try
        {
            std::cout << "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î: " << url << std::endl;
            int documentId = database_.savingDocument(url, result.title, result.cleanContent);
            if (documentId > 0 && !result.wordsFrequency.empty())
            {
                database_.savingWords(documentId, result.wordsFrequency);
                std::cout << "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î (ID: " << documentId << ")" << std::endl;
            }
        }
        catch (const std::exception& e)
        {
            std::cerr << "–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î " << url << ": " << e.what() << std::endl;
            return;
        }

        // –ï—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã, –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
        if (depth < config_.getSpiderMaxDepth())
        {
            try
            {
                std::cout << "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑: " << url << std::endl;
                std::vector<std::string> links = downloader_.extractLinks(html, url);
                std::cout << "–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: " << links.size() << std::endl;

                for (const auto& link : links)
                {
                    std::cout << "–î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å: " << link << std::endl;
                    addTask(link, depth + 1);
                }
            }
            catch (const std::exception& e)
            {
                std::cerr << "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ " << url << ": " << e.what() << std::endl;
            }
        }
    }
    catch (const std::exception& e)
    {
        std::cerr << "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã " << url << ": " << e.what() << std::endl;
    }
}

void Spider::start()
{
    stopRequested_ = false;

    // –°–æ–∑–¥–∞—ë–º –ø—É–ª –ø–æ—Ç–æ–∫–æ–≤
    unsigned int numThreads = std::thread::hardware_concurrency();
    if (numThreads == 0) numThreads = 2; // fallback

    std::cout << "\nüöÄ –ó–∞–ø—É—Å–∫ –ø–∞—É–∫–∞" << std::endl;
    std::cout << "   –ü–æ—Ç–æ–∫–æ–≤: " << numThreads << std::endl;
    std::cout << "   –ì–ª—É–±–∏–Ω–∞: " << config_.getSpiderMaxDepth() << std::endl;
    std::cout << "   –°—Ç–∞—Ä—Ç–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: " << config_.getSpiderStartUrl() << std::endl;

    for (unsigned int i = 0; i < numThreads; ++i)
    {
        workers_.emplace_back(&Spider::workerFunction, this);
    }
}

void Spider::stop()
{
    stopRequested_ = true;
    queueCV_.notify_all();

    for (auto& worker : workers_)
    {
        if (worker.joinable())
        {
            worker.join();
        }
    }

    workers_.clear();

    std::cout << "\nüõë –ü–∞—É–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω" << std::endl;
    std::cout << "   –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: " << pagesDownloaded_ << " —Å—Ç—Ä–∞–Ω–∏—Ü" << std::endl;
    std::cout << "   –í—Å–µ–≥–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: " << pagesIndexed_ << " —Å—Ç—Ä–∞–Ω–∏—Ü" << std::endl;
}

Spider::SpiderStats Spider::getStats() const
{
    SpiderStats stats;
    stats.totalDownloaded = pagesDownloaded_;
    stats.totalIndexed = pagesIndexed_;
    stats.activeWorkers = activeWorkers_;

    std::lock_guard<std::mutex> lock(queueMutex_);
    stats.queueSize = static_cast<int>(taskQueue_.size());

    return stats;
}

bool Spider::isRunning() const
{
    return activeWorkers_ > 0;
}